{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdfcd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vbharg4@AD/.conda/envs/vllm_test/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-23 02:36:25 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "# from config import PreferenceSamplerConfig\n",
    "# from data import PreferenceSampler, PreferenceSamplerConfig\n",
    "import json\n",
    "import pickle\n",
    "from data import PreferenceSampler, PreferenceSamplerConfig\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer, pipeline, GenerationConfig, AutoModelForCausalLM, set_seed\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "import gc\n",
    "\n",
    "# from uncertainty import UncertaintyConfig, get_uncertainity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b02e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = os.path.join('/home/vbharg4@AD', 'models')\n",
    "ds_id = os.path.join(os.getcwd(), \"ultrafeedback_iter1\")\n",
    "ref_model_path = os.path.join(MODEL_DIR, 'LLaMA3-SFT')\n",
    "reward_model_path = os.path.join(MODEL_DIR, 'FsfairX-LLaMA3-RM-v0.1')\n",
    "\n",
    "config = PreferenceSamplerConfig(dataset_dir=ds_id, ref_model_path=ref_model_path, rm_path=reward_model_path)\n",
    "pref_sampler = PreferenceSampler(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d434b37",
   "metadata": {},
   "source": [
    "20k prompts take about 2hr 30 mins and give us 80k responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d05119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-19 17:28:56 [utils.py:233] non-default args: {'gpu_memory_utilization': 0.75, 'disable_log_stats': True, 'enforce_eager': True, 'model': '/home/vbharg4@AD/models/LLaMA3-SFT'}\n",
      "INFO 11-19 17:28:56 [model.py:547] Resolved architecture: LlamaForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-19 17:28:57 [model.py:1510] Using max model len 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-19 17:28:58,105\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-19 17:28:58 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 11-19 17:28:58 [__init__.py:381] Cudagraph is disabled under eager mode\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3063542)\u001b[0;0m INFO 11-19 17:28:58 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3063542)\u001b[0;0m INFO 11-19 17:28:58 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/vbharg4@AD/models/LLaMA3-SFT', speculative_config=None, tokenizer='/home/vbharg4@AD/models/LLaMA3-SFT', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/vbharg4@AD/models/LLaMA3-SFT, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":0,\"local_cache_dir\":null}\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3063542)\u001b[0;0m INFO 11-19 17:29:05 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3063542)\u001b[0;0m WARNING 11-19 17:29:05 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3063542)\u001b[0;0m INFO 11-19 17:29:06 [gpu_model_runner.py:2602] Starting to load model /home/vbharg4@AD/models/LLaMA3-SFT...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3063542)\u001b[0;0m INFO 11-19 17:29:07 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3063542)\u001b[0;0m INFO 11-19 17:29:07 [cuda.py:366] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.23it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.22it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.23it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.63it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.45it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3063542)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3063542)\u001b[0;0m INFO 11-19 17:29:12 [default_loader.py:267] Loading weights took 2.80 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3063542)\u001b[0;0m INFO 11-19 17:29:12 [gpu_model_runner.py:2653] Model loading took 14.9596 GiB and 5.329133 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3063542)\u001b[0;0m INFO 11-19 17:29:16 [gpu_worker.py:298] Available KV cache memory: 43.21 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3063542)\u001b[0;0m INFO 11-19 17:29:16 [kv_cache_utils.py:1087] GPU KV cache size: 353,936 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3063542)\u001b[0;0m INFO 11-19 17:29:16 [kv_cache_utils.py:1091] Maximum concurrency for 8,192 tokens per request: 43.21x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3063542)\u001b[0;0m WARNING 11-19 17:29:17 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3063542)\u001b[0;0m INFO 11-19 17:29:17 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.67 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3063542)\u001b[0;0m INFO 11-19 17:29:17 [__init__.py:381] Cudagraph is disabled under eager mode\n",
      "INFO 11-19 17:29:18 [llm.py:306] Supported_tasks: ['generate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 20000/20000 [00:16<00:00, 1204.01it/s]\n",
      "Processed prompts: 100%|██████████| 80000/80000 [1:15:23<00:00, 17.68it/s, est. speed input: 2951.25 toks/s, output: 4153.88 toks/s] \n",
      "Adding requests: 100%|██████████| 20000/20000 [00:18<00:00, 1093.52it/s]\n",
      "Processed prompts: 100%|██████████| 80000/80000 [1:16:05<00:00, 17.52it/s, est. speed input: 2924.38 toks/s, output: 4061.69 toks/s] \n"
     ]
    }
   ],
   "source": [
    "fp_1 = os.path.join(os.getcwd(), 'output_policy_1.pkl')\n",
    "fp_2 = os.path.join(os.getcwd(), 'output_policy_2.pkl')\n",
    "\n",
    "skip_sampling = os.path.exists(fp_1) and os.path.exists(fp_2)\n",
    "\n",
    "if skip_sampling:\n",
    "    pass\n",
    "else:\n",
    "    outputs_policy_1, outputs_policy_2 = pref_sampler.generate_responses()\n",
    "\n",
    "    with open(os.path.join(os.getcwd(), 'output_policy_1.pkl'), mode='wb') as f:\n",
    "        pickle.dump(outputs_policy_1, f)\n",
    "\n",
    "    with open(os.path.join(os.getcwd(), 'output_policy_2.pkl'), mode='wb') as f:\n",
    "        pickle.dump(outputs_policy_2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab1e01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 86.84it/s]\n",
      "Device set to use cuda:0\n",
      "Ranking prompt 11 samples:   0%|          | 10/20000 [00:09<4:11:11,  1.33it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Ranking prompt 20000 samples: 100%|██████████| 20000/20000 [2:08:50<00:00,  2.59it/s]  \n"
     ]
    }
   ],
   "source": [
    "fp_3 = os.path.join(os.getcwd(), 'preference_samples.json')\n",
    "\n",
    "if not os.path.exists(fp_3):\n",
    "    samples = pref_sampler.rejection_sampling(outputs_policy_1, outputs_policy_2)\n",
    "\n",
    "    with open(os.path.join(os.getcwd(), 'preference_samples.json'), mode='w') as f:\n",
    "        json.dump(samples, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9fd1534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_response': '<|start_header_id|>user<|end_header_id|>\\nCan we conclude from \"A man wearing a grass skirt.\" that \"Is standing in the grass among a group of pigs on their backs near the coast.\"?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell A step-by-step solution is:\\n<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n1. The statement \"A man wearing a grass skirt\" tells us that there is a man wearing a grass skirt.\\n2. However, it does not provide any information about the man\\'s location or the surroundings.\\n3. The statement \"Is standing in the grass among a group of pigs on their backs near the coast\" describes a specific location and situation.\\n4. There is no information in the first statement that connects the man wearing a grass skirt to the location or situation described in the second statement.\\n5. Therefore, we cannot conclude that \"Is standing in the grass among a group of pigs on their backs near the coast\" based on the information provided in the first statement.\\n\\nThe answer is: no<|eot_id|>',\n",
       " 'worst_response': '<|start_header_id|>user<|end_header_id|>\\nCan we conclude from \"A man wearing a grass skirt.\" that \"Is standing in the grass among a group of pigs on their backs near the coast.\"?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell A step-by-step solution is:\\n<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\nno<|eot_id|>'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_key = list(samples.keys())[0]\n",
    "samples[first_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23547d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_response': '<|start_header_id|>user<|end_header_id|>\\nCan we conclude from \"A man wearing a grass skirt.\" that \"Is standing in the grass among a group of pigs on their backs near the coast.\"?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell A step-by-step solution is:\\n<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n1. The statement \"A man wearing a grass skirt\" tells us that there is a man wearing a grass skirt.\\n2. However, it does not provide any information about the man\\'s location or the surroundings.\\n3. The statement \"Is standing in the grass among a group of pigs on their backs near the coast\" describes a specific location and situation.\\n4. There is no information in the first statement that connects the man wearing a grass skirt to the location or situation described in the second statement.\\n5. Therefore, we cannot conclude that \"Is standing in the grass among a group of pigs on their backs near the coast\" based on the information provided in the first statement.\\n\\nThe answer is: no<|eot_id|>',\n",
       " 'worst_response': '<|start_header_id|>user<|end_header_id|>\\nCan we conclude from \"A man wearing a grass skirt.\" that \"Is standing in the grass among a group of pigs on their backs near the coast.\"?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell A step-by-step solution is:\\n<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\nno<|eot_id|>'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "first_key = list(samples.keys())[0]\n",
    "samples[first_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae6638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.getcwd(), '20k_responses', 'output_policy_1.pkl'), mode='rb') as f:\n",
    "    outputs_policy_1 = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(os.getcwd(), '20k_responses', 'output_policy_2.pkl'), mode='rb') as f:\n",
    "    outputs_policy_2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deda2a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = defaultdict(list)\n",
    "eot_id = '<|eot_id|>'\n",
    "\n",
    "check_assertion = True\n",
    "\n",
    "for output_policy_1, output_policy_2 in zip(outputs_policy_1, outputs_policy_2):\n",
    "    prompt = output_policy_1.prompt\n",
    "    if check_assertion:\n",
    "        assert prompt == output_policy_2.prompt\n",
    "        check_assertion = False\n",
    "\n",
    "    for out1, out2 in zip(output_policy_1.outputs, output_policy_2.outputs):\n",
    "        samples[prompt].append(prompt + out1.text + eot_id)\n",
    "        samples[prompt].append(prompt + out2.text + eot_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ca95fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|start_header_id|>user<|end_header_id|>\\nCan we conclude from \"A man wearing a grass skirt.\" that \"Is standing in the grass among a group of pigs on their backs near the coast.\"?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell A step-by-step solution is:\\n<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n1. The initial statement describes a man wearing a grass skirt.\\n2. This gives us a descriptive image of the man\\'s attire, but it doesn\\'t provide enough information to make conclusions about his surroundings or surroundings with animals.\\n3. The absence of details about the animals or location doesn\\'t allow us to deduce the situation involving pigs standing near the coast.\\n4. Therefore, we can\\'t conclusively say if he is standing in the grass among a group of pigs near the coast or not.\\n\\nConclusion: \\n- not possible to tell<|eot_id|>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples['<|start_header_id|>user<|end_header_id|>\\nCan we conclude from \"A man wearing a grass skirt.\" that \"Is standing in the grass among a group of pigs on their backs near the coast.\"?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell A step-by-step solution is:\\n<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4646e793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 73.99it/s]\n",
      "Device set to use cuda:0\n",
      "Ranking prompt 11 samples:   0%|          | 10/20000 [00:09<4:11:45,  1.32it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Ranking prompt 20000 samples: 100%|██████████| 20000/20000 [2:09:21<00:00,  2.58it/s]  \n"
     ]
    }
   ],
   "source": [
    "samples = pref_sampler.rejection_sampling(outputs_policy_1, outputs_policy_2)\n",
    "\n",
    "with open(os.path.join(os.getcwd(),'20k_responses', 'preference_samples.json'), mode='w') as f:\n",
    "    json.dump(samples, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be9d7ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.getcwd(), '20k_responses', 'preference_samples.json')) as f:\n",
    "        preference_pairs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa7c77be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>user<|end_header_id|>\n",
      "Can we conclude from \"A man wearing a grass skirt.\" that \"Is standing in the grass among a group of pigs on their backs near the coast.\"?\n",
      "Options:\n",
      "- yes\n",
      "- no\n",
      "- it is not possible to tell A step-by-step solution is:\n",
      "<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "Can we conclude from \"A man wearing a grass skirt.\" that \"Is standing in the grass among a group of pigs on their backs near the coast.\"?\n",
      "Options:\n",
      "- yes\n",
      "- no\n",
      "- it is not possible to tell A step-by-step solution is:\n",
      "<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "1. The statement \"A man wearing a grass skirt\" tells us that there is a man wearing a grass skirt.\n",
      "2. However, it does not provide any information about the man's location or the surroundings.\n",
      "3. The statement \"Is standing in the grass among a group of pigs on their backs near the coast\" describes a specific location and situation.\n",
      "4. There is no information in the first statement that connects the man wearing a grass skirt to the location or situation described in the second statement.\n",
      "5. Therefore, we cannot conclude that \"Is standing in the grass among a group of pigs on their backs near the coast\" based on the information provided in the first statement.\n",
      "\n",
      "The answer is: no<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "Can we conclude from \"A man wearing a grass skirt.\" that \"Is standing in the grass among a group of pigs on their backs near the coast.\"?\n",
      "Options:\n",
      "- yes\n",
      "- no\n",
      "- it is not possible to tell A step-by-step solution is:\n",
      "<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "no<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "prompt, pairs = list(preference_pairs.items())[0]\n",
    "best_response, worst_response = pairs['best_response'], pairs['worst_response']\n",
    "print(prompt)\n",
    "print(best_response)\n",
    "print(worst_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "861a2acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pref_sampler.dataset[0]['context_messages'][0]['content'] == prompt.removeprefix('<|start_header_id|>user<|end_header_id|>\\n').removesuffix('<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cde4b9",
   "metadata": {},
   "source": [
    "Don't need to convert back responses and prompts to the role, content format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "272988da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl.data_utils import maybe_apply_chat_template\n",
    "tokenizer = AutoTokenizer.from_pretrained(pref_sampler.ref_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ac9d725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '<|start_header_id|>user<|end_header_id|>\\n<|start_header_id|>user<|end_header_id|>\\nCan we conclude from \"A man wearing a grass skirt.\" that \"Is standing in the grass among a group of pigs on their backs near the coast.\"?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell A step-by-step solution is:\\n<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n',\n",
       " 'chosen': '<|start_header_id|>user<|end_header_id|>\\nCan we conclude from \"A man wearing a grass skirt.\" that \"Is standing in the grass among a group of pigs on their backs near the coast.\"?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell A step-by-step solution is:\\n<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n1. The statement \"A man wearing a grass skirt\" tells us that there is a man wearing a grass skirt.\\n2. However, it does not provide any information about the man\\'s location or the surroundings.\\n3. The statement \"Is standing in the grass among a group of pigs on their backs near the coast\" describes a specific location and situation.\\n4. There is no information in the first statement that connects the man wearing a grass skirt to the location or situation described in the second statement.\\n5. Therefore, we cannot conclude that \"Is standing in the grass among a group of pigs on their backs near the coast\" based on the information provided in the first statement.\\n\\nThe answer is: no<|eot_id|><|eot_id|>\\n',\n",
       " 'rejected': '<|start_header_id|>user<|end_header_id|>\\nCan we conclude from \"A man wearing a grass skirt.\" that \"Is standing in the grass among a group of pigs on their backs near the coast.\"?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell A step-by-step solution is:\\n<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\nno<|eot_id|><|eot_id|>\\n'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maybe_apply_chat_template({\n",
    "    'prompt': [{'role': 'user', 'content': prompt}],\n",
    "    'chosen': [{'role': 'assistant', 'content': best_response}],\n",
    "    'rejected': [{'role': 'assistant', 'content': worst_response}]\n",
    "}, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c866e716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '<|start_header_id|>user<|end_header_id|>\\nCan we conclude from \"A man wearing a grass skirt.\" that \"Is standing in the grass among a group of pigs on their backs near the coast.\"?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell A step-by-step solution is:\\n<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n',\n",
       " 'chosen': '1. The statement \"A man wearing a grass skirt\" tells us that there is a man wearing a grass skirt.\\n2. However, it does not provide any information about the man\\'s location or the surroundings.\\n3. The statement \"Is standing in the grass among a group of pigs on their backs near the coast\" describes a specific location and situation.\\n4. There is no information in the first statement that connects the man wearing a grass skirt to the location or situation described in the second statement.\\n5. Therefore, we cannot conclude that \"Is standing in the grass among a group of pigs on their backs near the coast\" based on the information provided in the first statement.\\n\\nThe answer is: no<|eot_id|>\\n',\n",
       " 'rejected': 'no<|eot_id|>\\n'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maybe_apply_chat_template({\n",
    "    'prompt': [{'role': 'user', 'content': prompt.removeprefix('<|start_header_id|>user<|end_header_id|>\\n').removesuffix('<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n')}],\n",
    "    'chosen': [{'role': 'assistant', 'content': best_response.removeprefix(prompt).removesuffix('<|eot_id|>')}],\n",
    "    'rejected': [{'role': 'assistant', 'content': worst_response.removeprefix(prompt).removesuffix('<|eot_id|>')}]\n",
    "}, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81e01e5",
   "metadata": {},
   "source": [
    "Just remove the prompt prefix from the chosen and rejected responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "640eee9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': ['<|start_header_id|>user<|end_header_id|>\\nCan we conclude from \"A man wearing a grass skirt.\" that \"Is standing in the grass among a group of pigs on their backs near the coast.\"?\\nOptions:\\n- yes\\n- no\\n- it is not possible to tell A step-by-step solution is:\\n<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n'],\n",
       " 'chosen': ['1. The statement \"A man wearing a grass skirt\" tells us that there is a man wearing a grass skirt.\\n2. However, it does not provide any information about the man\\'s location or the surroundings.\\n3. The statement \"Is standing in the grass among a group of pigs on their backs near the coast\" describes a specific location and situation.\\n4. There is no information in the first statement that connects the man wearing a grass skirt to the location or situation described in the second statement.\\n5. Therefore, we cannot conclude that \"Is standing in the grass among a group of pigs on their backs near the coast\" based on the information provided in the first statement.\\n\\nThe answer is: no<|eot_id|>'],\n",
       " 'rejected': ['no<|eot_id|>']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maybe_apply_chat_template({\n",
    "    'prompt': [prompt],\n",
    "    'chosen': [best_response.removeprefix(prompt)],\n",
    "    'rejected': [worst_response.removeprefix(prompt)]\n",
    "}, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e769dc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncertainity_scores_efficient_v2(preference_pairs: dict[str: dict[str: str]], first_k: int = 100) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    TODO: Improve the throughput for higher batches of responses. Right now it is just a pair of responses.\n",
    "    \"\"\"\n",
    "    model = LlamaForCausalLM.from_pretrained(ref_model_path, device_map='cuda:0', dtype=torch.bfloat16)\n",
    "    model.requires_grad_(False)\n",
    "    \n",
    "    if model.generation_config.temperature is None:\n",
    "        model.generation_config.temperature = 1.0\n",
    "\n",
    "    # already set\n",
    "    model.generation_config.do_sample = True\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ref_model_path)\n",
    "\n",
    "    model.set_attn_implementation('eager')\n",
    "\n",
    "    samples_attn_scores = []\n",
    "\n",
    "    if first_k <= 0:\n",
    "        first_k = len(preference_pairs)\n",
    "\n",
    "    pbar_prefernces = tqdm(list(preference_pairs.items())[:first_k])\n",
    "\n",
    "    for idx, (prompt, pairs) in enumerate(pbar_prefernces, start=1):\n",
    "        pbar_prefernces.set_description(f'Prompt {idx} uncertainty')\n",
    "        best_response, worst_response = pairs['best_response'], pairs['worst_response']\n",
    "\n",
    "        inputs = tokenizer([best_response, worst_response], return_tensors='pt', truncation=True, max_length=2048, padding=True)\n",
    "\n",
    "        kwargs = {\n",
    "            \"input_ids\": inputs['input_ids'].to(model.device),\n",
    "            \"attention_mask\": inputs['attention_mask'].to(model.device),\n",
    "            \"use_cache\": False,\n",
    "            \"past_key_values\": None,\n",
    "            \"output_attentions\": True,\n",
    "            \"output_hidden_states\": False,\n",
    "            \"return_dict\": True,\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(**kwargs)\n",
    "\n",
    "        attns = output.attentions\n",
    "\n",
    "        # No squeezing\n",
    "        attn_ = torch.stack([x.to(torch.float32).detach().cpu() for x in attns])\n",
    "\n",
    "        prompt_len = len(tokenizer(prompt)['input_ids'])\n",
    "        mask_sum = inputs['attention_mask'].sum(dim=-1)\n",
    "\n",
    "        is_single_sequence = not mask_sum.shape\n",
    "\n",
    "        if is_single_sequence:\n",
    "            # mask sum is a scalar.\n",
    "            span_idx = [[prompt_len, mask_sum]]\n",
    "        else:\n",
    "            span_idx = [[prompt_len, total_len] for total_len in mask_sum]\n",
    "\n",
    "        sample_attn_scores = []\n",
    "\n",
    "        for sample_idx in range(len(span_idx)):\n",
    "            # layers_attn_scores = []\n",
    "            # attn_ -> layers x samples x heads x seq. len x seq. len\n",
    "            start, end = span_idx[sample_idx]\n",
    "            # sample_KER -> layers x heads x span x span\n",
    "            sample_KER = attn_[:, sample_idx, 1:, start: end, start: end]\n",
    "            # log det mean over tokens (after taking diagonal, it's the last dim) and then sum over heads (2nd dim from the left side).\n",
    "            layers_attn_scores = torch.diagonal(sample_KER, dim1=-2, dim2=-1).log().mean(dim=-1).sum(dim=1)\n",
    "            sample_attn_scores.append(layers_attn_scores)\n",
    "\n",
    "        samples_attn_scores.append(sample_attn_scores)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    del model, tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return np.array(samples_attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5e79678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncertainity_scores_efficient(preference_pairs: dict[str: dict[str: str]], first_k: int = 100) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    TODO: Improve the throughput for higher batches of responses. Right now it is just a pair of responses.\n",
    "    \"\"\"\n",
    "    model = LlamaForCausalLM.from_pretrained(ref_model_path, device_map='cuda:0', dtype=torch.bfloat16)\n",
    "    model.requires_grad_(False)\n",
    "    \n",
    "    if model.generation_config.temperature is None:\n",
    "        model.generation_config.temperature = 1.0\n",
    "\n",
    "    # already set\n",
    "    model.generation_config.do_sample = True\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ref_model_path)\n",
    "\n",
    "    model.set_attn_implementation('eager')\n",
    "\n",
    "    samples_attn_scores = []\n",
    "\n",
    "    if first_k <= 0:\n",
    "        first_k = len(preference_pairs)\n",
    "\n",
    "    pbar_prefernces = tqdm(list(preference_pairs.items())[:first_k])\n",
    "\n",
    "    for idx, (prompt, pairs) in enumerate(pbar_prefernces, start=1):\n",
    "        pbar_prefernces.set_description(f'Prompt {idx} uncertainty')\n",
    "        best_response, worst_response = pairs['best_response'], pairs['worst_response']\n",
    "\n",
    "        inputs = tokenizer([best_response, worst_response], return_tensors='pt', truncation=True, max_length=2048, padding=True)\n",
    "\n",
    "        kwargs = {\n",
    "            \"input_ids\": inputs['input_ids'].to(model.device),\n",
    "            \"attention_mask\": inputs['attention_mask'].to(model.device),\n",
    "            \"use_cache\": False,\n",
    "            \"past_key_values\": None,\n",
    "            \"output_attentions\": True,\n",
    "            \"output_hidden_states\": False,\n",
    "            \"return_dict\": True,\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(**kwargs)\n",
    "\n",
    "        attns = output.attentions\n",
    "\n",
    "        # No squeezing\n",
    "        attn_ = torch.stack([x.to(torch.float32).detach().cpu() for x in attns])\n",
    "\n",
    "        prompt_len = len(tokenizer(prompt)['input_ids'])\n",
    "        mask_sum = inputs['attention_mask'].sum(dim=-1)\n",
    "\n",
    "        is_single_sequence = not mask_sum.shape\n",
    "\n",
    "        if is_single_sequence:\n",
    "            # mask sum is a scalar.\n",
    "            span_idx = [[prompt_len, mask_sum]]\n",
    "        else:\n",
    "            span_idx = [[prompt_len, total_len] for total_len in mask_sum]\n",
    "\n",
    "        sample_attn_scores = []\n",
    "\n",
    "        for sample_idx in range(len(span_idx)):\n",
    "            # layers_attn_scores = []\n",
    "            # attn_ -> layers x samples x heads x seq. len x seq. len\n",
    "            start, end = span_idx[sample_idx]\n",
    "            # sample_KER -> layers x heads x span x span\n",
    "            sample_KER = attn_[:, sample_idx, :, start: end, start: end]\n",
    "            # log det mean over tokens (after taking diagonal, it's the last dim) and then sum over heads (2nd dim from the left side).\n",
    "            layers_attn_scores = torch.diagonal(sample_KER, dim1=-2, dim2=-1).log().mean(dim=-1).sum(dim=1)\n",
    "            sample_attn_scores.append(layers_attn_scores)\n",
    "\n",
    "        samples_attn_scores.append(sample_attn_scores)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    del model, tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return np.array(samples_attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ab0f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncertainity_scores(preference_pairs: dict[str: dict[str: str]], first_k: int = 100) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    TODO: Improve the throughput for higher batches of responses. Right now it is just a pair of responses.\n",
    "    \"\"\"\n",
    "    model = LlamaForCausalLM.from_pretrained(ref_model_path, device_map='cuda:0', dtype=torch.bfloat16)\n",
    "    model.requires_grad_(False)\n",
    "    \n",
    "    if model.generation_config.temperature is None:\n",
    "        model.generation_config.temperature = 1.0\n",
    "\n",
    "    # already set\n",
    "    model.generation_config.do_sample = True\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ref_model_path)\n",
    "\n",
    "    model.set_attn_implementation('eager')\n",
    "\n",
    "    samples_attn_scores = []\n",
    "\n",
    "    if first_k <= 0:\n",
    "        first_k = len(preference_pairs)\n",
    "\n",
    "    pbar_prefernces = tqdm(list(preference_pairs.items())[:first_k])\n",
    "\n",
    "    for idx, (prompt, pairs) in enumerate(pbar_prefernces, start=1):\n",
    "        pbar_prefernces.set_description(f'Prompt {idx} uncertainty')\n",
    "        best_response, worst_response = pairs['best_response'], pairs['worst_response']\n",
    "\n",
    "        inputs = tokenizer([best_response, worst_response], return_tensors='pt', truncation=True, max_length=2048, padding=True)\n",
    "\n",
    "        kwargs = {\n",
    "            \"input_ids\": inputs['input_ids'].to(model.device),\n",
    "            \"attention_mask\": inputs['attention_mask'].to(model.device),\n",
    "            \"use_cache\": False,\n",
    "            \"past_key_values\": None,\n",
    "            \"output_attentions\": True,\n",
    "            \"output_hidden_states\": False,\n",
    "            \"return_dict\": True,\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(**kwargs)\n",
    "\n",
    "        attns = output.attentions\n",
    "\n",
    "        # No squeezing\n",
    "        attn_ = [x.to(torch.float32).detach().cpu() for x in attns]\n",
    "\n",
    "        prompt_len = len(tokenizer(prompt)['input_ids'])\n",
    "        mask_sum = inputs['attention_mask'].sum(dim=-1)\n",
    "\n",
    "        is_single_sequence = not mask_sum.shape\n",
    "\n",
    "        if is_single_sequence:\n",
    "            # mask sum is a scalar.\n",
    "            span_idx = [[prompt_len, mask_sum]]\n",
    "        else:\n",
    "            span_idx = [[prompt_len, total_len] for total_len in mask_sum]\n",
    "\n",
    "        sample_attn_scores = []\n",
    "\n",
    "        for sample_idx in range(len(span_idx)):\n",
    "            layers_attn_scores = []\n",
    "            for block_idx in range(len(attns)):\n",
    "                eigen_score = 0.0\n",
    "                for head_idx in range(1, len(attn_[block_idx][sample_idx])):\n",
    "                    # attn_ is a tuple of attention matrices. With size Batch x head x Sequence len x Sequence len\n",
    "                    _KER = attn_[block_idx][sample_idx][head_idx]\n",
    "                    start, end = span_idx[sample_idx]\n",
    "                    KER = _KER[start: end, start: end]\n",
    "\n",
    "                    eigen_score += KER.diagonal().log().mean()\n",
    "                layers_attn_scores.append(eigen_score.item())\n",
    "\n",
    "            sample_attn_scores.append(layers_attn_scores)\n",
    "\n",
    "        samples_attn_scores.append(sample_attn_scores)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    del model, tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return np.array(samples_attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da5fd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_scores_path = os.path.join(os.getcwd(), 'scores_10k.npz')\n",
    "if not os.path.exists(uncertainty_scores_path):\n",
    "    scores_10000 = get_uncertainity_scores(preference_pairs, first_k=10000)\n",
    "    scores_2000 = scores_10000[:2000]\n",
    "    scores_200 = scores_10000[:200]\n",
    "\n",
    "    np.savez_compressed(os.path.join(os.getcwd(), 'scores_10k.npz'), scores_10000)\n",
    "    np.savez_compressed(os.path.join(os.getcwd(), 'scores_2k.npz'), scores_2000)\n",
    "    np.savez_compressed(os.path.join(os.getcwd(), 'scores_200.npz'), scores_200)\n",
    "else:\n",
    "    scores_10000 = np.load(os.path.join(os.getcwd(), 'scores_10k.npz'))['arr_0']\n",
    "    scores_2000 = np.load(os.path.join(os.getcwd(), 'scores_2k.npz'))['arr_0']\n",
    "    scores_200 = np.load(os.path.join(os.getcwd(), 'scores_200.npz'))['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76699bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.09it/s]\n",
      "Prompt 2000 uncertainty: 100%|██████████| 2000/2000 [59:00<00:00,  1.77s/it]  \n"
     ]
    }
   ],
   "source": [
    "scores_2000 = get_uncertainity_scores(preference_pairs, first_k=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80c998ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.06it/s]\n",
      "Prompt 10000 uncertainty: 100%|██████████| 10000/10000 [5:11:09<00:00,  1.87s/it] \n"
     ]
    }
   ],
   "source": [
    "scores_10000 = get_uncertainity_scores(preference_pairs, first_k=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6a8239",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_tokenizer = AutoTokenizer.from_pretrained(reward_model_path)\n",
    "\n",
    "rm_pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=config.rm_path,\n",
    "    device=f'cuda:{config.rm_device}',\n",
    "    tokenizer=rm_tokenizer,\n",
    "    model_kwargs={\"dtype\": torch.bfloat16},\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea1ed2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics_on_uncertainity_scores(scores: np.ndarray, threshold: float = 1.0):\n",
    "    # ratio of log eigen values \n",
    "    sample_attention_block_uncertainity_mask = (scores[:, 1, :] / scores[:, 0, :]) < threshold\n",
    "\n",
    "    sample_attention_block_uncertainity_count = sample_attention_block_uncertainity_mask.sum()\n",
    "    sample_attention_block_certainity_count = (~sample_attention_block_uncertainity_mask).sum()\n",
    "\n",
    "    sample_mean_uncertainity_mask = (scores[:, 1, :] / scores[:, 0, :]).mean(axis=-1) < threshold\n",
    "\n",
    "    sample_mean_uncertainity_count = sample_mean_uncertainity_mask.sum()\n",
    "    sample_mean_certainity_count = (~sample_mean_uncertainity_mask).sum()\n",
    "\n",
    "    print(f'For threshold value {threshold} {len(scores)} samples had')\n",
    "\n",
    "    print(f'Attention block counts where uncertainity is present   : {sample_attention_block_uncertainity_count}')\n",
    "    print(f'Attention block counts where certainity is present     : {sample_attention_block_certainity_count}')\n",
    "\n",
    "    print(f'Samples detected where uncertainity is present         : {sample_mean_uncertainity_count}')\n",
    "    print(f'Samples detected where certainity is present           : {sample_mean_certainity_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee642bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For threshold value 1.5 10000 samples had\n",
      "Attention block counts where uncertainity is present   : 319815\n",
      "Attention block counts where certainity is present     : 185\n",
      "Samples detected where uncertainity is present         : 9996\n",
      "Samples detected where certainity is present           : 4\n",
      "********************************************************************************\n",
      "For threshold value 1.4 10000 samples had\n",
      "Attention block counts where uncertainity is present   : 319680\n",
      "Attention block counts where certainity is present     : 320\n",
      "Samples detected where uncertainity is present         : 9995\n",
      "Samples detected where certainity is present           : 5\n",
      "********************************************************************************\n",
      "For threshold value 1.3 10000 samples had\n",
      "Attention block counts where uncertainity is present   : 319324\n",
      "Attention block counts where certainity is present     : 676\n",
      "Samples detected where uncertainity is present         : 9988\n",
      "Samples detected where certainity is present           : 12\n",
      "********************************************************************************\n",
      "For threshold value 1.2 10000 samples had\n",
      "Attention block counts where uncertainity is present   : 318284\n",
      "Attention block counts where certainity is present     : 1716\n",
      "Samples detected where uncertainity is present         : 9952\n",
      "Samples detected where certainity is present           : 48\n",
      "********************************************************************************\n",
      "For threshold value 1.1 10000 samples had\n",
      "Attention block counts where uncertainity is present   : 315239\n",
      "Attention block counts where certainity is present     : 4761\n",
      "Samples detected where uncertainity is present         : 9872\n",
      "Samples detected where certainity is present           : 128\n",
      "********************************************************************************\n",
      "For threshold value 1.05 10000 samples had\n",
      "Attention block counts where uncertainity is present   : 307138\n",
      "Attention block counts where certainity is present     : 12862\n",
      "Samples detected where uncertainity is present         : 9722\n",
      "Samples detected where certainity is present           : 278\n",
      "********************************************************************************\n",
      "For threshold value 1.005 10000 samples had\n",
      "Attention block counts where uncertainity is present   : 240689\n",
      "Attention block counts where certainity is present     : 79311\n",
      "Samples detected where uncertainity is present         : 7988\n",
      "Samples detected where certainity is present           : 2012\n",
      "********************************************************************************\n",
      "For threshold value 1.00005 10000 samples had\n",
      "Attention block counts where uncertainity is present   : 218814\n",
      "Attention block counts where certainity is present     : 101186\n",
      "Samples detected where uncertainity is present         : 7219\n",
      "Samples detected where certainity is present           : 2781\n",
      "********************************************************************************\n",
      "For threshold value 1.0 10000 samples had\n",
      "Attention block counts where uncertainity is present   : 205874\n",
      "Attention block counts where certainity is present     : 114126\n",
      "Samples detected where uncertainity is present         : 6814\n",
      "Samples detected where certainity is present           : 3186\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for threshold in [1.5, 1.4, 1.3, 1.2, 1.1, 1.05, 1.005, 1.00005 ,1.0]:\n",
    "    print_metrics_on_uncertainity_scores(scores_10000, threshold=threshold)\n",
    "    print('*' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1ef0414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For threshold value 1.0 10000 samples had\n",
      "Attention block counts where uncertainity is present   : 205874\n",
      "Attention block counts where certainity is present     : 114126\n",
      "Samples detected where uncertainity is present         : 6814\n",
      "Samples detected where certainity is present           : 3186\n"
     ]
    }
   ],
   "source": [
    "print_metrics_on_uncertainity_scores(scores_10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ae08a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For threshold value 1.5 2000 samples had\n",
      "Attention block counts where uncertainity is present   : 63988\n",
      "Attention block counts where certainity is present     : 12\n",
      "Samples detected where uncertainity is present         : 2000\n",
      "Samples detected where certainity is present           : 0\n",
      "********************************************************************************\n",
      "For threshold value 1.4 2000 samples had\n",
      "Attention block counts where uncertainity is present   : 63961\n",
      "Attention block counts where certainity is present     : 39\n",
      "Samples detected where uncertainity is present         : 1999\n",
      "Samples detected where certainity is present           : 1\n",
      "********************************************************************************\n",
      "For threshold value 1.3 2000 samples had\n",
      "Attention block counts where uncertainity is present   : 63894\n",
      "Attention block counts where certainity is present     : 106\n",
      "Samples detected where uncertainity is present         : 1999\n",
      "Samples detected where certainity is present           : 1\n",
      "********************************************************************************\n",
      "For threshold value 1.2 2000 samples had\n",
      "Attention block counts where uncertainity is present   : 63711\n",
      "Attention block counts where certainity is present     : 289\n",
      "Samples detected where uncertainity is present         : 1990\n",
      "Samples detected where certainity is present           : 10\n",
      "********************************************************************************\n",
      "For threshold value 1.1 2000 samples had\n",
      "Attention block counts where uncertainity is present   : 63208\n",
      "Attention block counts where certainity is present     : 792\n",
      "Samples detected where uncertainity is present         : 1980\n",
      "Samples detected where certainity is present           : 20\n",
      "********************************************************************************\n",
      "For threshold value 1.005 2000 samples had\n",
      "Attention block counts where uncertainity is present   : 47885\n",
      "Attention block counts where certainity is present     : 16115\n",
      "Samples detected where uncertainity is present         : 1587\n",
      "Samples detected where certainity is present           : 413\n",
      "********************************************************************************\n",
      "For threshold value 1.00005 2000 samples had\n",
      "Attention block counts where uncertainity is present   : 43513\n",
      "Attention block counts where certainity is present     : 20487\n",
      "Samples detected where uncertainity is present         : 1445\n",
      "Samples detected where certainity is present           : 555\n",
      "********************************************************************************\n",
      "For threshold value 1.0 2000 samples had\n",
      "Attention block counts where uncertainity is present   : 40846\n",
      "Attention block counts where certainity is present     : 23154\n",
      "Samples detected where uncertainity is present         : 1361\n",
      "Samples detected where certainity is present           : 639\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for threshold in [1.5, 1.4, 1.3, 1.2, 1.1, 1.005, 1.00005 ,1.0]:\n",
    "    print_metrics_on_uncertainity_scores(scores_2000, threshold=threshold)\n",
    "    print('*' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e481896e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 2, 32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_200.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0a248ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-150.84275818, -138.31906128, -196.58979797, -156.59933472,\n",
       "       -145.96615601, -150.52931213, -129.75682068, -131.40637207,\n",
       "       -125.60826111, -134.65904236, -132.68510437, -136.05809021,\n",
       "       -104.24207306, -132.39122009, -118.20938873, -125.3552475 ,\n",
       "       -125.41088104, -129.94017029, -153.50650024, -151.42243958,\n",
       "       -148.80877686, -140.93400574, -146.02807617, -152.01097107,\n",
       "       -161.16046143, -160.43185425, -146.24899292, -153.10655212,\n",
       "       -115.73773956, -150.01724243, -121.30107117,  -93.4316864 ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_200[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6ff886e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-150.26776123, -134.5160675 , -154.78442383, -146.59112549,\n",
       "       -130.68878174, -134.30236816, -109.23898315, -116.79162598,\n",
       "        -93.26016235, -102.11379242, -102.10191345, -106.66228485,\n",
       "        -77.10712433,  -95.91215515,  -80.74992371,  -99.81895447,\n",
       "       -102.32080841, -115.67222595, -130.74130249, -127.99694061,\n",
       "       -129.70353699, -125.09207916, -133.72634888, -147.05903625,\n",
       "       -130.23358154, -132.94363403, -121.65923309, -130.96412659,\n",
       "        -94.67223358, -119.02914429,  -95.164711  ,  -70.59238434])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_200[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ccb48b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01s/it]\n",
      "Prompt 200 uncertainty: 100%|██████████| 200/200 [05:38<00:00,  1.69s/it]\n"
     ]
    }
   ],
   "source": [
    "scores_200 = get_uncertainity_scores(preference_pairs, first_k=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a908bd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For threshold value 1.5 200 samples had\n",
      "Attention block counts where uncertainity is present   : 6390\n",
      "Attention block counts where certainity is present     : 10\n",
      "Samples detected where uncertainity is present         : 200\n",
      "Samples detected where certainity is present           : 0\n",
      "********************************************************************************\n",
      "For threshold value 1.4 200 samples had\n",
      "Attention block counts where uncertainity is present   : 6373\n",
      "Attention block counts where certainity is present     : 27\n",
      "Samples detected where uncertainity is present         : 199\n",
      "Samples detected where certainity is present           : 1\n",
      "********************************************************************************\n",
      "For threshold value 1.3 200 samples had\n",
      "Attention block counts where uncertainity is present   : 6360\n",
      "Attention block counts where certainity is present     : 40\n",
      "Samples detected where uncertainity is present         : 199\n",
      "Samples detected where certainity is present           : 1\n",
      "********************************************************************************\n",
      "For threshold value 1.2 200 samples had\n",
      "Attention block counts where uncertainity is present   : 6326\n",
      "Attention block counts where certainity is present     : 74\n",
      "Samples detected where uncertainity is present         : 198\n",
      "Samples detected where certainity is present           : 2\n",
      "********************************************************************************\n",
      "For threshold value 1.1 200 samples had\n",
      "Attention block counts where uncertainity is present   : 6269\n",
      "Attention block counts where certainity is present     : 131\n",
      "Samples detected where uncertainity is present         : 196\n",
      "Samples detected where certainity is present           : 4\n",
      "********************************************************************************\n",
      "For threshold value 1.005 200 samples had\n",
      "Attention block counts where uncertainity is present   : 4743\n",
      "Attention block counts where certainity is present     : 1657\n",
      "Samples detected where uncertainity is present         : 162\n",
      "Samples detected where certainity is present           : 38\n",
      "********************************************************************************\n",
      "For threshold value 1.00005 200 samples had\n",
      "Attention block counts where uncertainity is present   : 4318\n",
      "Attention block counts where certainity is present     : 2082\n",
      "Samples detected where uncertainity is present         : 149\n",
      "Samples detected where certainity is present           : 51\n",
      "********************************************************************************\n",
      "For threshold value 1.0 200 samples had\n",
      "Attention block counts where uncertainity is present   : 4092\n",
      "Attention block counts where certainity is present     : 2308\n",
      "Samples detected where uncertainity is present         : 142\n",
      "Samples detected where certainity is present           : 58\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for threshold in [1.5, 1.4, 1.3, 1.2, 1.1, 1.005, 1.00005 ,1.0]:\n",
    "    print_metrics_on_uncertainity_scores(scores_200, threshold=threshold)\n",
    "    print('*' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c4ca31da",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(os.path.join(os.getcwd(), 'scores_10k.npz'), scores_10000)\n",
    "np.savez_compressed(os.path.join(os.getcwd(), 'scores_2k.npz'), scores_2000)\n",
    "np.savez_compressed(os.path.join(os.getcwd(), 'scores_200.npz'), scores_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e275df4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(scores_10000[:200] == scores_200).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c4d1ba",
   "metadata": {},
   "source": [
    "Could not test the <it>efficiency</it> properly, many people were running their code on the same GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c103d7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.20s/it]\n",
      "Prompt 200 uncertainty: 100%|██████████| 200/200 [07:10<00:00,  2.15s/it]\n"
     ]
    }
   ],
   "source": [
    "scores_200_efficient = get_uncertainity_scores_efficient_v2(preference_pairs, first_k=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da0a65ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.False_"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(scores_200_efficient == scores_200).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d00d00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.08it/s]\n",
      "Prompt 200 uncertainty: 100%|██████████| 200/200 [07:07<00:00,  2.14s/it]\n"
     ]
    }
   ],
   "source": [
    "scores_200_efficient = get_uncertainity_scores_efficient(preference_pairs, first_k=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24d4c7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 2, 32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_200_efficient.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2289d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.False_"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(scores_200_efficient == scores_200).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c1eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (scores_200[:, 1, :] / scores_200[:, 0, :]) < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e83bb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(4092)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc8e2706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(2308)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(~mask).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477a694e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "mean_mask = (scores_200[:, 1, :] / scores_200[:, 0, :]).mean(axis=-1) < 1\n",
    "print(mean_mask.sum())\n",
    "print((~mean_mask).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c91b18",
   "metadata": {},
   "source": [
    "Uncertaintity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bcc7a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_eig_prod(attns, layer_num=15, tok_lens=[], use_toklens=True):\n",
    "    \"\"\"Compute an eigenvalue-based attention score by analyzing attention matrices.\n",
    "\n",
    "    This function takes the attention matrices of a given layer and for each sample,\n",
    "    computes the mean log of the diagonal elements (assumed to be eigenvalues) across\n",
    "    all attention heads. Slices are applied if `tok_lens` is used.\n",
    "\n",
    "    Args:\n",
    "        attns (list): A list of tuples, each containing attention matrices for all layers\n",
    "            and heads for a single sample.\n",
    "        layer_num (int, optional): The layer index to evaluate. Defaults to 15.\n",
    "        tok_lens (list, optional): A list of (start, end) indices for each sample to slice\n",
    "            the attention matrices. Defaults to [].\n",
    "        use_toklens (bool, optional): Whether to slice the attention matrices using `tok_lens`.\n",
    "            Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        np.array: An array of computed attention-based eigenvalue scores for each sample.\n",
    "    \"\"\"\n",
    "    attn_scores = []\n",
    "\n",
    "    for i in range(len(attns)):  # iterating over number of samples\n",
    "        eigscore = 0.0\n",
    "        for attn_head_num in range(len(attns[i][layer_num])):  # iterating over number of attn heads\n",
    "            # attns[i][layer_num][j] is of size seq_len x seq_len\n",
    "            Sigma = attns[i][layer_num][attn_head_num]\n",
    "\n",
    "            print(Sigma.shape)\n",
    "\n",
    "            if use_toklens and tok_lens[i]:\n",
    "                i1, i2 = tok_lens[i][0], tok_lens[i][1]\n",
    "                Sigma = Sigma[i1:i2, i1:i2]\n",
    "\n",
    "            eigscore += torch.log(torch.diagonal(Sigma, 0)).mean()\n",
    "        attn_scores.append(eigscore.item())\n",
    "\n",
    "    return np.stack(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "254b3a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(ref_model_path, device_map='cuda', dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(ref_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7353fed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>user<|end_header_id|>\n",
      "Can we conclude from \"A man wearing a grass skirt.\" that \"Is standing in the grass among a group of pigs on their backs near the coast.\"?\n",
      "Options:\n",
      "- yes\n",
      "- no\n",
      "- it is not possible to tell A step-by-step solution is:\n",
      "<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "1. The statement \"A man wearing a grass skirt\" tells us that there is a man wearing a grass skirt.\n",
      "2. However, it does not provide any information about the man's location or the surroundings.\n",
      "3. The statement \"Is standing in the grass among a group of pigs on their backs near the coast\" describes a specific location and situation.\n",
      "4. There is no information in the first statement that connects the man wearing a grass skirt to the location or situation described in the second statement.\n",
      "5. Therefore, we cannot conclude that \"Is standing in the grass among a group of pigs on their backs near the coast\" based on the information provided in the first statement.\n",
      "\n",
      "The answer is: no<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(best_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "21af7d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000, 128006,    882, 128007,    198,   6854,    584,  32194,    505,\n",
       "            330,     32,    893,  12512,    264,  16763,  38380,   1210,    430,\n",
       "            330,   3957,  11509,    304,    279,  16763,   4315,    264,   1912,\n",
       "            315,  49910,    389,    872,  28678,   3221,    279,  13962,   1210,\n",
       "           5380,   3883,    512,     12,  10035,    198,     12,    912,    198,\n",
       "             12,    433,    374,    539,   3284,    311,   3371,    362,   3094,\n",
       "          14656,  30308,   6425,    374,    512, 128009,    198, 128006,  78191,\n",
       "         128007,    198,     16,     13,    578,   5224,    330,     32,    893,\n",
       "          12512,    264,  16763,  38380,      1,  10975,    603,    430,   1070,\n",
       "            374,    264,    893,  12512,    264,  16763,  38380,    627,     17,\n",
       "             13,   4452,     11,    433,   1587,    539,   3493,    904,   2038,\n",
       "            922,    279,    893,    596,   3813,    477,    279,  40190,    627,\n",
       "             18,     13,    578,   5224,    330,   3957,  11509,    304,    279,\n",
       "          16763,   4315,    264,   1912,    315,  49910,    389,    872,  28678,\n",
       "           3221,    279,  13962,      1,  16964,    264,   3230,   3813,    323,\n",
       "           6671,    627,     19,     13,   2684,    374,    912,   2038,    304,\n",
       "            279,   1176,   5224,    430,  34161,    279,    893,  12512,    264,\n",
       "          16763,  38380,    311,    279,   3813,    477,   6671,   7633,    304,\n",
       "            279,   2132,   5224,    627,     20,     13,  15636,     11,    584,\n",
       "           4250,  32194,    430,    330,   3957,  11509,    304,    279,  16763,\n",
       "           4315,    264,   1912,    315,  49910,    389,    872,  28678,   3221,\n",
       "            279,  13962,      1,   3196,    389,    279,   2038,   3984,    304,\n",
       "            279,   1176,   5224,    382,    791,   4320,    374,     25,    912,\n",
       "         128009],\n",
       "        [128000, 128006,    882, 128007,    198,   6854,    584,  32194,    505,\n",
       "            330,     32,    893,  12512,    264,  16763,  38380,   1210,    430,\n",
       "            330,   3957,  11509,    304,    279,  16763,   4315,    264,   1912,\n",
       "            315,  49910,    389,    872,  28678,   3221,    279,  13962,   1210,\n",
       "           5380,   3883,    512,     12,  10035,    198,     12,    912,    198,\n",
       "             12,    433,    374,    539,   3284,    311,   3371,    362,   3094,\n",
       "          14656,  30308,   6425,    374,    512, 128009,    198, 128006,  78191,\n",
       "         128007,    198,   2201, 128009, 128001, 128001, 128001, 128001, 128001,\n",
       "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "         128001]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer([best_response, worst_response], return_tensors='pt', truncation=True, max_length=2048, padding=True)\n",
    "# inputs = tokenizer([best_response], return_tensors='pt', truncation=True, max_length=2048, padding=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ccd0926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(prompt)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "176475f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"input_ids\": inputs['input_ids'].to(model.device),\n",
    "    \"attention_mask\": inputs['attention_mask'].to(model.device),\n",
    "    \"use_cache\": False,\n",
    "    \"past_key_values\": None,\n",
    "    \"output_attentions\": True,\n",
    "    \"output_hidden_states\": False,\n",
    "    \"return_dict\": True,\n",
    "}\n",
    "\n",
    "model.set_attn_implementation('eager')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(**kwargs)\n",
    "\n",
    "attns = output.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58c2f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n",
      "torch.Size([208, 208])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([-142.86560059]),\n",
       " array([-202.32208252]),\n",
       " array([-162.06225586]),\n",
       " array([-151.41825867]),\n",
       " array([-156.10256958]),\n",
       " array([-133.93257141]),\n",
       " array([-133.43083191]),\n",
       " array([-131.08973694]),\n",
       " array([-137.83486938]),\n",
       " array([-133.9058075]),\n",
       " array([-140.9239502]),\n",
       " array([-107.86325073]),\n",
       " array([-136.85749817]),\n",
       " array([-122.06723785]),\n",
       " array([-127.99515533]),\n",
       " array([-130.8465271]),\n",
       " array([-135.32292175]),\n",
       " array([-157.84783936]),\n",
       " array([-156.59591675]),\n",
       " array([-151.01907349]),\n",
       " array([-144.39160156]),\n",
       " array([-151.51086426]),\n",
       " array([-156.84384155]),\n",
       " array([-166.85919189]),\n",
       " array([-166.90963745]),\n",
       " array([-148.17816162]),\n",
       " array([-158.60186768]),\n",
       " array([-118.64774323]),\n",
       " array([-154.79603577]),\n",
       " array([-123.10443115]),\n",
       " array([-94.35379028])]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No squeezing\n",
    "attn_ = [x.to(torch.float32).detach().cpu() for x in attns]\n",
    "\n",
    "prompt_len = len(tokenizer(prompt)['input_ids'])\n",
    "mask_sum = inputs['attention_mask'].sum(dim=-1)\n",
    "\n",
    "is_single_sequence = not mask_sum.shape\n",
    "\n",
    "if is_single_sequence:\n",
    "    # mask sum is a scalar.\n",
    "    span_idx = [[prompt_len, mask_sum]]\n",
    "else:\n",
    "    span_idx = [[prompt_len, total_len] for total_len in mask_sum]\n",
    "\n",
    "sample_attn_scores = []\n",
    "attn_scores = []\n",
    "for head_idx in range(1, len(attn_)):\n",
    "    attn_scores.append(get_attn_eig_prod([attn_], layer_num=head_idx, tok_lens=span_idx))\n",
    "\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "67331014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 208, 208])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attns[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f0b8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-150.84275818, -138.31906128, -196.58979797, -156.59933472,\n",
       "        -145.96615601, -150.52931213, -129.75682068, -131.40637207,\n",
       "        -125.60826111, -134.65904236, -132.68510437, -136.05809021,\n",
       "        -104.24207306, -132.39122009, -118.20938873, -125.3552475 ,\n",
       "        -125.41088104, -129.94017029, -153.50650024, -151.42243958,\n",
       "        -148.80877686, -140.93400574, -146.02807617, -152.01097107,\n",
       "        -161.16046143, -160.43185425, -146.24899292, -153.10655212,\n",
       "        -115.73773956, -150.01724243, -121.30107117,  -93.4316864 ],\n",
       "       [-150.26776123, -134.5160675 , -154.78442383, -146.59112549,\n",
       "        -130.68878174, -134.30236816, -109.23898315, -116.79162598,\n",
       "         -93.26016235, -102.11379242, -102.10191345, -106.66228485,\n",
       "         -77.10712433,  -95.91215515,  -80.74992371,  -99.81895447,\n",
       "        -102.32080841, -115.67222595, -130.74130249, -127.99694061,\n",
       "        -129.70353699, -125.09207916, -133.72634888, -147.05903625,\n",
       "        -130.23358154, -132.94363403, -121.65923309, -130.96412659,\n",
       "         -94.67223358, -119.02914429,  -95.164711  ,  -70.59238434]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No squeezing\n",
    "attn_ = [x.to(torch.float32).detach().cpu() for x in attns]\n",
    "\n",
    "prompt_len = len(tokenizer(prompt)['input_ids'])\n",
    "mask_sum = inputs['attention_mask'].sum(dim=-1)\n",
    "\n",
    "is_single_sequence = not mask_sum.shape\n",
    "\n",
    "if is_single_sequence:\n",
    "    # mask sum is a scalar.\n",
    "    span_idx = [[prompt_len, mask_sum]]\n",
    "else:\n",
    "    span_idx = [[prompt_len, total_len] for total_len in mask_sum]\n",
    "\n",
    "sample_attn_scores = []\n",
    "\n",
    "for sample_idx in range(len(span_idx)):\n",
    "    layers_attn_scores = []\n",
    "    for block_idx in range(len(attns)):\n",
    "        eigen_score = 0.0\n",
    "        for head_idx in range(1, len(attn_[block_idx][sample_idx])):\n",
    "            # attn_ is a tuple of attention matrices. With size Batch x Layer x Sequence len x Sequence len\n",
    "            _KER = attn_[block_idx][sample_idx][head_idx]\n",
    "            start, end = span_idx[sample_idx]\n",
    "            KER = _KER[start: end, start: end]\n",
    "\n",
    "            eigen_score += KER.diagonal().log().mean()\n",
    "        layers_attn_scores.append(eigen_score.item())\n",
    "\n",
    "    sample_attn_scores.append(layers_attn_scores)\n",
    "\n",
    "sample_attn_scores = np.array(sample_attn_scores)\n",
    "sample_attn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0283060",
   "metadata": {},
   "source": [
    "Ratio of log det eigen values is lower for 'bad' to 'good' responses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "76a6b8e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9961881 , 0.97250564, 0.78734718, 0.93609035, 0.89533619,\n",
       "       0.89220077, 0.84187469, 0.88878206, 0.74246838, 0.75831367,\n",
       "       0.76950547, 0.78394666, 0.73969293, 0.72446009, 0.68310922,\n",
       "       0.7962886 , 0.81588462, 0.89019605, 0.8516988 , 0.84529704,\n",
       "       0.87161214, 0.8875933 , 0.91575779, 0.96742383, 0.80809884,\n",
       "       0.82866108, 0.83186373, 0.85537898, 0.81798931, 0.79343642,\n",
       "       0.78453315, 0.75555079])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_attn_scores[1] / sample_attn_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e658fa36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((sample_attn_scores[1] / sample_attn_scores[0]) < 1).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
