# ===== ScriptArguments fields =====
ref_model: "RLHFlow/LLaMA3-SFT"
reward_model: "sfairXC/FsfairX-LLaMA3-RM-v0.1"
train_dir: "RLHFlow/ultrafeedback_iter1"

ref_device: 0
rm_device: 1

eos_padding: true
margin_scale: 1.0
sanity_check: false
max_training_samples: -1
choose_type: "max_random"
ignore_bias_buffers: false
eot_token: "<|eot_id|>"
len_penalty: 0.0
corrupt_preferences: false

# ===== PreferenceSamplerConfig-related fields =====
samples_drawn_size: 20
num_return_sequences: 4
generation_seed: 42
dataset_dir: "RLHFlow/ultrafeedback_iter1"
ref_gpu_utlization: 0.5
rm_batch_size: 8

# ===== DPOConfig fields (from trl) =====
output_dir: "checkpoints/dpo_run"
eval_strategy: 'steps'
gradient_accumulation_steps: 2
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  "use_reentrant": false
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
num_train_epochs: 2
learning_rate: 5e-7
max_grad_norm: 0.5
beta: 0.1
max_length: 2048
max_prompt_length: 1024
loss_type: "sigmoid"
save_strategy: "no"
remove_unused_columns: false
logging_steps: 10
report_to: []

# ===== ModelConfig fields (from trl) =====
model_name_or_path: null
dtype: "bfloat16"
trust_remote_code: false
use_peft: false